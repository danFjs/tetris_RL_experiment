{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import A2C,PPO\n",
    "from tetris_gym.envs.tetris_gym import TetrisGym\n",
    "from tetris_gym.wrappers.observation import ExtendedObservationWrapper\n",
    "from tetris_gym.utils.eval_utils import evaluate, create_videos\n",
    "from agent import agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TetrisGym(width=10, height=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = ExtendedObservationWrapper(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('board',\n",
       "              array([[1, 0, 1, 1, 1, 0, 0, 1, 1, 1],\n",
       "                     [1, 0, 1, 1, 0, 1, 1, 1, 0, 1],\n",
       "                     [1, 1, 0, 0, 0, 0, 1, 0, 1, 1],\n",
       "                     [1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [1, 0, 0, 1, 1, 1, 0, 1, 1, 1],\n",
       "                     [1, 1, 0, 1, 0, 1, 0, 0, 1, 1],\n",
       "                     [1, 1, 1, 1, 1, 1, 0, 0, 1, 1],\n",
       "                     [0, 0, 1, 1, 1, 1, 0, 1, 0, 1],\n",
       "                     [0, 0, 1, 1, 0, 0, 1, 1, 1, 1],\n",
       "                     [0, 1, 1, 0, 1, 1, 0, 0, 1, 0],\n",
       "                     [0, 1, 0, 1, 1, 1, 0, 0, 0, 0],\n",
       "                     [1, 0, 0, 1, 0, 0, 1, 1, 0, 0],\n",
       "                     [0, 1, 1, 1, 0, 0, 0, 1, 1, 0],\n",
       "                     [1, 0, 1, 1, 0, 0, 1, 1, 0, 1],\n",
       "                     [1, 1, 1, 0, 1, 1, 1, 0, 0, 0],\n",
       "                     [1, 1, 1, 0, 1, 1, 0, 1, 1, 1],\n",
       "                     [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                     [1, 1, 1, 1, 1, 0, 0, 1, 1, 0],\n",
       "                     [1, 1, 1, 0, 1, 0, 1, 0, 0, 0],\n",
       "                     [1, 1, 1, 1, 0, 0, 0, 0, 0, 1]], dtype=uint8)),\n",
       "             ('bumps', array([ 4,  8, 15,  6,  5, 12, 11,  6, 15])),\n",
       "             ('heights', array([ 8,  7,  1, 17, 18,  0,  8, 13, 14,  9])),\n",
       "             ('piece', 6)])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.observation_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "obs, reward, done, info = env.step((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs, reward, done, info = env.step((0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'board': array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 1, 1, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [0, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "        [1, 1, 0, 0, 0, 0, 0, 0, 0, 0]]),\n",
       " 'piece': 6,\n",
       " 'heights': array([9, 9, 8, 5, 0, 0, 0, 0, 0, 0], dtype=int64),\n",
       " 'bumps': [0, -1, -3, -5, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 243,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 11, 9, 31]"
      ]
     },
     "execution_count": 256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.get_state_properties(obs['board'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Whopperwhopperjuniorwhopper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomRewardWrapper(gym.Wrapper):\n",
    "\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    # Felülírjük a környezet beépített step függvényét\n",
    "    def step(self, action):\n",
    "\n",
    "        # Meghívjuk az eredeti környezet step függvényét\n",
    "        obs, reward, done, info = self.env.step(action)\n",
    "\n",
    "        # Új jutalmat számítunk, minél jobban eldől az inga vagy elmozog a\n",
    "        # kocsi, annál jobban büntetjük\n",
    "        line_reward = 0.76\n",
    "        hole_reward = 0.36\n",
    "        bumpiness_reward = 0.18\n",
    "        height_reward = 0.51\n",
    "        lines_cleared = self.env.get_state_properties(obs['board'])[0]\n",
    "        holes = self.env.get_state_properties(obs['board'])[1]\n",
    "        bumpiness = self.env.get_state_properties(obs['board'])[2]\n",
    "        height = self.env.get_state_properties(obs['board'])[3]\n",
    "        reward += (- (hole_reward* holes )\n",
    "                  - ( bumpiness_reward*bumpiness )\n",
    "                  - (height_reward*height)\n",
    "                  + (line_reward*lines_cleared)+2\n",
    "        )\n",
    "        return obs, reward, done, info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 588,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = TetrisGym(width=10, height=20)\n",
    "env = ExtendedObservationWrapper(env)\n",
    "env = CustomRewardWrapper(env)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy modification"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "import gym\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### simple\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size = env.observation_space['board'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "piece_size = env.observation_space['piece']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discrete(7)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "piece_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "a2_size = env.action_space[1].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "a1_size = env.action_space[0].n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_size = board_size[0]*board_size[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size,piece_size,heights_size,bumps_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 614,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.action_space.sample()[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoofyAhhNet(nn.Module):\n",
    "    def __init__(self, s_size, a1_size, a2_size, h_size):\n",
    "        super(GoofyAhhNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size)\n",
    "\n",
    "        self.fch1 = nn.Linear(h_size,h_size)\n",
    "\n",
    "        \n",
    "        self.fc21 = nn.Linear(h_size, a1_size)\n",
    "        self.fc22 = nn.Linear(h_size, a2_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fch1(x))\n",
    "        out1 = self.fc21(x)\n",
    "        out2 = self.fc22(x)\n",
    "        return [F.softmax(out1, dim=1),F.softmax(out2, dim=1)]\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state['board'].flatten()).float().unsqueeze(0).to(device)\n",
    "        probs1 = self.forward(state)[0].cpu()\n",
    "        probs2 = self.forward(state)[1].cpu()\n",
    "        m1 = Categorical(probs1)\n",
    "        action1 = m1.sample()\n",
    "        m2 = Categorical(probs2)\n",
    "        action2 = m2.sample()\n",
    "        return (action1.item(),action2.item()),(m1.log_prob(action1),m2.log_prob(action2))\n",
    "    def predict(self,state):\n",
    "        action =self.act(state)\n",
    "        act1,_ = action[0]\n",
    "        act2,_ = action[1]\n",
    "        return act1,act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()['board'].flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a2_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = GoofyAhhNet(s_size, a1_size,a2_size,100).to(device)\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1,r2 = policy_net(torch.from_numpy(state['board'].flatten()).float().unsqueeze(0).to(device))\n",
    "r1 = r1.max(1)[1].view(1, 1)\n",
    "r2 = r2.max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6]])"
      ]
     },
     "execution_count": 621,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1]])"
      ]
     },
     "execution_count": 622,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "r2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'max'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[618], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m debug_policy \u001b[39m=\u001b[39m GoofyAhhNet(s_size, a1_size,a2_size, \u001b[39m10\u001b[39m)\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m----> 2\u001b[0m debug_policy(torch\u001b[39m.\u001b[39;49mfrom_numpy(env\u001b[39m.\u001b[39;49mreset()[\u001b[39m'\u001b[39;49m\u001b[39mboard\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mflatten())\u001b[39m.\u001b[39;49mfloat()\u001b[39m.\u001b[39;49munsqueeze(\u001b[39m0\u001b[39;49m)\u001b[39m.\u001b[39;49mto(device))\u001b[39m.\u001b[39;49mmax(\u001b[39m1\u001b[39m)[\u001b[39m1\u001b[39m]\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'max'"
     ]
    }
   ],
   "source": [
    "debug_policy = GoofyAhhNet(s_size, a1_size,a2_size, 10).to(device)\n",
    "debug_policy(torch.from_numpy(env.reset()['board'].flatten()).float().unsqueeze(0).to(device)).max(1)[1].view(1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GoofyAhhNet(\n",
       "  (fc1): Linear(in_features=200, out_features=10, bias=True)\n",
       "  (fc21): Linear(in_features=10, out_features=10, bias=True)\n",
       "  (fc22): Linear(in_features=10, out_features=4, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "debug_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs1 = []\n",
    "        saved_log_probs2 = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            actions, log_probs = policy.act(state)\n",
    "            action1,action2 = actions\n",
    "            log_prob1,log_prob2 = log_probs\n",
    "            saved_log_probs1.append(log_prob1)\n",
    "            saved_log_probs2.append(log_prob2)\n",
    "            state, reward, done, _ = env.step((action1,action2))\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        \n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        \n",
    "        for log_prob1,log_prob2, disc_return in zip(saved_log_probs1,saved_log_probs2 , returns):\n",
    "            policy_loss.append(-log_prob1 *-log_prob2* disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_hyperparams = {\n",
    "    \"h_size\": 30,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"state_space\": 200,\n",
    "    \"action_space\": [a1_size,a2_size],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExtendedGoofyAhhNet(nn.Module):\n",
    "    def __init__(self, s_size, a1_size, a2_size,h_size):\n",
    "        super(ExtendedGoofyAhhNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(s_size, h_size*2)\n",
    "        self.fch1 = nn.Linear(h_size*2,h_size*2)\n",
    "        self.fch2 = nn.Linear(h_size*2,h_size*2)\n",
    "        self.fch3 = nn.Linear(h_size*2,h_size)\n",
    "        self.fc21 = nn.Linear(h_size, a1_size)\n",
    "        self.fc22 = nn.Linear(h_size, a2_size)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fch1(x))\n",
    "        x = F.relu(self.fch2(x))\n",
    "        x = F.relu(self.fch3(x))\n",
    "        out1 = self.fc21(x)\n",
    "        out2 = self.fc22(x)\n",
    "        return [F.softmax(out1, dim=1),F.softmax(out2, dim=1)]\n",
    "    \n",
    "    def act(self, state):\n",
    "        board_state = torch.from_numpy(state['board'].flatten()).float().unsqueeze(0).to(device)\n",
    "        piece_state = torch.from_numpy(np.array([[state['piece']]])).float().to(device)\n",
    "        height_state = torch.from_numpy(state['heights']).float().unsqueeze(0).to(device)\n",
    "        bumps_state = torch.from_numpy(np.array(state['bumps'])).float().unsqueeze(0).to(device)\n",
    "        state = torch.from_numpy(np.concatenate([board_state,piece_state,height_state,bumps_state],axis=1)).to(device)\n",
    "        probs1 = self.forward(state)[0].cpu()\n",
    "        probs2 = self.forward(state)[1].cpu()\n",
    "        m1 = Categorical(probs1)\n",
    "        action1 = m1.sample()\n",
    "        m2 = Categorical(probs2)\n",
    "        action2 = m2.sample()\n",
    "        return (action1.item(),action2.item()),(m1.log_prob(action1),m2.log_prob(action2))\n",
    "    def predict(self,state):\n",
    "        action =self.act(state)\n",
    "        act1,_ = action[0]\n",
    "        act2,_ = action[1]\n",
    "        return act1,act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinforce(policy, optimizer, n_training_episodes, max_t, gamma, print_every):\n",
    "    # Help us to calculate the score during the training\n",
    "    scores_deque = deque(maxlen=100)\n",
    "    scores = []\n",
    "    # Line 3 of pseudocode\n",
    "    for i_episode in range(1, n_training_episodes+1):\n",
    "        saved_log_probs1 = []\n",
    "        saved_log_probs2 = []\n",
    "        rewards = []\n",
    "\n",
    "        state = env.reset()\n",
    "        # Line 4 of pseudocode\n",
    "        for t in range(max_t):\n",
    "            actions, log_probs = policy.act(state)\n",
    "            action1,action2 = actions\n",
    "            log_prob1,log_prob2 = log_probs\n",
    "            saved_log_probs1.append(log_prob1)\n",
    "            saved_log_probs2.append(log_prob2)\n",
    "            state, reward, done, _ = env.step((action1,action2))\n",
    "            rewards.append(reward)\n",
    "            if done:\n",
    "                break \n",
    "        scores_deque.append(sum(rewards))\n",
    "        scores.append(sum(rewards))\n",
    "\n",
    "        returns = deque(maxlen=max_t) \n",
    "        n_steps = len(rewards) \n",
    "        for t in range(n_steps)[::-1]:\n",
    "            disc_return_t = (returns[0] if len(returns)>0 else 0)\n",
    "            returns.appendleft( gamma*disc_return_t + rewards[t]   )    \n",
    "            \n",
    "        \n",
    "        eps = np.finfo(np.float32).eps.item()\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + eps)\n",
    "        \n",
    "        # Line 7:\n",
    "        policy_loss = []\n",
    "        \n",
    "        for log_prob1,log_prob2, disc_return in zip(saved_log_probs1,saved_log_probs2 , returns):\n",
    "            policy_loss.append(-log_prob1 *-log_prob2* disc_return)\n",
    "        policy_loss = torch.cat(policy_loss).sum()\n",
    "        \n",
    "        # Line 8: PyTorch prefers gradient descent \n",
    "        optimizer.zero_grad()\n",
    "        policy_loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i_episode % print_every == 0:\n",
    "            print('Episode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_deque)))\n",
    "        \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "metadata": {},
   "outputs": [],
   "source": [
    "board_size =env.observation_space.sample()['board'].shape[0] * env.observation_space.sample()['board'].shape[1]\n",
    "piece_size = 1\n",
    "height_size = env.observation_space.sample()['heights'].shape[0]\n",
    "bumps_size = env.observation_space.sample()['bumps'].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "height_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_hyperparams = {\n",
    "    \"h_size\": 100,\n",
    "    \"n_training_episodes\": 1000,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1500,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-2,\n",
    "    \"state_space\": board_size+piece_size+height_size+bumps_size,\n",
    "    \"action_space\": [a1_size,a2_size],\n",
    "}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convolutional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 572,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GoofyAhhConvolutionalNet(nn.Module):\n",
    "    def __init__(self, s_size, a1_size, a2_size, h_size):\n",
    "        super(GoofyAhhConvolutionalNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 1, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act1 = nn.ReLU()\n",
    "        self.drop1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv2d(1, 1, kernel_size=(3,3), stride=1, padding=1)\n",
    "        self.act2 = nn.ReLU()\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=(2, 2))\n",
    "\n",
    "        self.flat = nn.Flatten()\n",
    "\n",
    "        self.fc3 = nn.Linear(50, 10)\n",
    "        self.act3 = nn.ReLU()\n",
    "        self.drop3 = nn.Dropout(0.5)\n",
    "        self.fch = nn.Linear(10,h_size)\n",
    "        self.fc21 = nn.Linear(h_size,a1_size)\n",
    "        self.fc22 = nn.Linear(h_size,a2_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # input 3x32x32, output 32x32x32\n",
    "        x = self.act1(self.conv1(x))\n",
    "        x = self.drop1(x)\n",
    "        # input 32x32x32, output 32x32x32\n",
    "        x = self.act2(self.conv2(x))\n",
    "        # input 32x32x32, output 32x16x16\n",
    "        x = self.pool2(x)\n",
    "        # input 32x16x16, output 8192\n",
    "        x = self.flat(x)\n",
    "        # input 8192, output 512\n",
    "        x = self.act3(self.fc3(x))\n",
    "        x = self.drop3(x)\n",
    "        # input 512, output 10\n",
    "        x = F.relu(self.fch(x))\n",
    "        out1 = self.fc21(x)\n",
    "        out2 = self.fc22(x)\n",
    "        \n",
    "        return [F.softmax(out1, dim=1),F.softmax(out2, dim=1)]\n",
    "    \n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state['board']).float().unsqueeze(0).to(device)\n",
    "        probs1 = self.forward(state)[0].cpu()\n",
    "        probs2 = self.forward(state)[1].cpu()\n",
    "        m1 = Categorical(probs1)\n",
    "        action1 = m1.sample((1,))\n",
    "        m2 = Categorical(probs2)\n",
    "        action2 = m2.sample((1,))\n",
    "        return (action1.item(),action2.item()),(m1.log_prob(action1),m2.log_prob(action2))\n",
    "    def predict(self,state):\n",
    "        action =self.act(state)\n",
    "        act1,_ = action[0]\n",
    "        act2,_ = action[1]\n",
    "        return act1,act2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.from_numpy(env.observation_space.sample()['board'].flatten()).float().unsqueeze(0).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "metadata": {},
   "outputs": [],
   "source": [
    "o = torch.from_numpy(env.observation_space.sample()['board']).float().unsqueeze(0).to(device)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## teach"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_hyperparams = {\n",
    "    \"h_size\": 256,\n",
    "    \"n_training_episodes\": 1500,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-4,\n",
    "    \"state_space\": 200,\n",
    "    \"action_space\": [a1_size,a2_size],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_net = GoofyAhhNet(tetris_hyperparams[\"state_space\"], tetris_hyperparams[\"action_space\"][0],tetris_hyperparams[\"action_space\"][1], tetris_hyperparams[\"h_size\"]).to(device)\n",
    "target_net = GoofyAhhNet(tetris_hyperparams[\"state_space\"], tetris_hyperparams[\"action_space\"][0],tetris_hyperparams[\"action_space\"][1], tetris_hyperparams[\"h_size\"]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "optimizer = optim.AdamW(policy_net.parameters(), lr=tetris_hyperparams['lr'], amsgrad=True)\n",
    "steps_done = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_policy = GoofyAhhNet(tetris_hyperparams[\"state_space\"], tetris_hyperparams[\"action_space\"][0],tetris_hyperparams[\"action_space\"][1], tetris_hyperparams[\"h_size\"]).to(device)\n",
    "tetris_optimizer = optim.Adam(tetris_policy.parameters(), lr=tetris_hyperparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -124.36\n",
      "Episode 200\tAverage Score: -117.26\n",
      "Episode 300\tAverage Score: -118.34\n",
      "Episode 400\tAverage Score: -118.34\n",
      "Episode 500\tAverage Score: -116.20\n",
      "Episode 600\tAverage Score: -117.43\n",
      "Episode 700\tAverage Score: -118.29\n",
      "Episode 800\tAverage Score: -114.48\n",
      "Episode 900\tAverage Score: -115.47\n",
      "Episode 1000\tAverage Score: -118.56\n",
      "Episode 1100\tAverage Score: -117.16\n",
      "Episode 1200\tAverage Score: -116.31\n",
      "Episode 1300\tAverage Score: -115.82\n",
      "Episode 1400\tAverage Score: -114.84\n",
      "Episode 1500\tAverage Score: -116.86\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(tetris_policy,\n",
    "                   tetris_optimizer,\n",
    "                   tetris_hyperparams[\"n_training_episodes\"], \n",
    "                   tetris_hyperparams[\"max_t\"],\n",
    "                   tetris_hyperparams[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extended"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_policy = ExtendedGoofyAhhNet(tetris_hyperparams[\"state_space\"], tetris_hyperparams[\"action_space\"][0],tetris_hyperparams[\"action_space\"][1], tetris_hyperparams[\"h_size\"]).to(device)\n",
    "tetris_optimizer = optim.Adam(tetris_policy.parameters(), lr=tetris_hyperparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 974.17\n",
      "Episode 200\tAverage Score: 843.17\n",
      "Episode 300\tAverage Score: 850.02\n",
      "Episode 400\tAverage Score: 844.39\n",
      "Episode 500\tAverage Score: 836.65\n",
      "Episode 600\tAverage Score: 837.89\n",
      "Episode 700\tAverage Score: 860.49\n",
      "Episode 800\tAverage Score: 836.40\n",
      "Episode 900\tAverage Score: 849.62\n",
      "Episode 1000\tAverage Score: 844.54\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(tetris_policy,\n",
    "                   tetris_optimizer,\n",
    "                   tetris_hyperparams[\"n_training_episodes\"], \n",
    "                   tetris_hyperparams[\"max_t\"],\n",
    "                   tetris_hyperparams[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 592,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_hyperparams = {\n",
    "    \"h_size\": 30,\n",
    "    \"n_training_episodes\": 1500,\n",
    "    \"n_evaluation_episodes\": 10,\n",
    "    \"max_t\": 1000,\n",
    "    \"gamma\": 1.0,\n",
    "    \"lr\": 1e-1,\n",
    "    \"state_space\": 200,\n",
    "    \"action_space\": [a1_size,a2_size],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 590,
   "metadata": {},
   "outputs": [],
   "source": [
    "tetris_policy = GoofyAhhConvolutionalNet(tetris_hyperparams[\"state_space\"], tetris_hyperparams[\"action_space\"][0],tetris_hyperparams[\"action_space\"][1], tetris_hyperparams[\"h_size\"]).to(device)\n",
    "tetris_optimizer = optim.Adam(tetris_policy.parameters(), lr=tetris_hyperparams[\"lr\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: -109.59\n",
      "Episode 200\tAverage Score: -109.97\n",
      "Episode 300\tAverage Score: -111.03\n",
      "Episode 400\tAverage Score: -114.07\n",
      "Episode 500\tAverage Score: -110.27\n",
      "Episode 600\tAverage Score: -107.24\n",
      "Episode 700\tAverage Score: -105.89\n",
      "Episode 800\tAverage Score: -105.30\n",
      "Episode 900\tAverage Score: -107.65\n",
      "Episode 1000\tAverage Score: -108.63\n",
      "Episode 1100\tAverage Score: -106.32\n",
      "Episode 1200\tAverage Score: -107.32\n",
      "Episode 1300\tAverage Score: -109.20\n",
      "Episode 1400\tAverage Score: -108.56\n",
      "Episode 1500\tAverage Score: -106.09\n"
     ]
    }
   ],
   "source": [
    "scores = reinforce(tetris_policy,\n",
    "                   tetris_optimizer,\n",
    "                   tetris_hyperparams[\"n_training_episodes\"], \n",
    "                   tetris_hyperparams[\"max_t\"],\n",
    "                   tetris_hyperparams[\"gamma\"], \n",
    "                   100)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tetris_gym.utils.eval_utils import evaluate_agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: -931.1\n"
     ]
    }
   ],
   "source": [
    "score = evaluate_agent(env,tetris_policy, 10)\n",
    "print(\"Score: {}\".format(score))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defa Solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 18.6     |\n",
      "|    ep_rew_mean     | 759      |\n",
      "| time/              |          |\n",
      "|    fps             | 493      |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 4        |\n",
      "|    total_timesteps | 2048     |\n",
      "---------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x22cc4c62590>"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PPO('MultiInputPolicy',  env, verbose=1, seed=42)\n",
    "model.learn(total_timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score: 396.17999999999995\n"
     ]
    }
   ],
   "source": [
    "# Kiértékelés 10 véletlen környezetben\n",
    "score = evaluate(env,model, 10)\n",
    "print(\"Score: {}\".format(score))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video maker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting imageio-ffmpeg\n",
      "  Using cached imageio_ffmpeg-0.4.8-py3-none-win_amd64.whl (22.6 MB)\n",
      "Installing collected packages: imageio-ffmpeg\n",
      "Successfully installed imageio-ffmpeg-0.4.8\n"
     ]
    }
   ],
   "source": [
    "!pip install imageio-ffmpeg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tetris_gym.utils.eval_utils import create_agent_videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-129.435"
      ]
     },
     "execution_count": 602,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_agent_videos(env, tetris_policy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
